{"cells":[{"cell_type":"markdown","source":["# Product Search Relevance in E-commerce"],"metadata":{}},{"cell_type":"markdown","source":["# Outline \n#### - Introduction\n#### - Data preprocessing \n#### - Feature Engineering\n#### - Modeling \n#### - Evaluation and Discussion"],"metadata":{}},{"cell_type":"markdown","source":["# Introduction \nIn the world of E-commerce, an accurate product match based on the search word input from the user is extremely essential. In a way that, if the user can be directly led to the products that they are looking for, their user experience could be greatly enhanced and so will the company be able to make more profits.\n## Background and Motivation\nIn text-based search area, the common problems would be how to extract useful information out of the unstructured data and how to efficiently recommend the most relevant products from millions of products. In this project, we use manually graded relevance score to train several machine learning models, so that the models could later be used to measure the relavance between search term and product pairs. \nManual calculation is taking the average score of three human raters, using criteria like : a search for \"AA battery\" would be considered highly relevant to a pack of size AA batteries (relevance = 3), mildly relevant to a cordless drill battery (relevance = 2), and not relevant to a snow shovel (relevance = 1). \nBe to able to predict the relevance score between search word and product description, we would be able to recommend the customers with the most relavent products that they are searching for. This would also provide suggestions for websites on how they should adjust their product title or product descriptions for the best search."],"metadata":{}},{"cell_type":"markdown","source":["# Data-source \nIn this project, home improvement products data from Home Depot and user search input data will be analyzed. A pipeline to predict the relevance for each pair of the query and the product will then be generated by us. \n\nhttps://www.kaggle.com/c/home-depot-product-search-relevance/data"],"metadata":{}},{"cell_type":"markdown","source":["# Import data and libraries"],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\nimport numpy as np\n\nimport gensim\nimport gensim.parsing.preprocessing as gsp\n\nimport nltk\nnltk.download('all')\nfrom gensim import utils\nfrom nltk.stem import PorterStemmer \nfrom nltk.corpus import stopwords\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.master(\"local[24]\").getOrCreate()\n\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import desc\nfrom pyspark.sql.types import StringType, IntegerType, ArrayType, FloatType, MapType, DoubleType\n\nfrom itertools import product\nfrom collections import defaultdict\n\nfrom scipy.spatial.distance import euclidean, cosine\nimport pulp\nimport re"],"metadata":{"id":"Uv-paxb3bhds","colab_type":"code","colab":{}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["# Read Data"],"metadata":{"id":"tUy9blO2J3QO","colab_type":"text"}},{"cell_type":"code","source":["# File location and type\nfile_location = \"/FileStore/tables/depot_train.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\ntitle_df = spark.read.format(file_type) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"delimiter\", delimiter) \\\n  .option(\"escape\", \"\\\"\") \\\n  .load(file_location)\n\nfile_location = \"/FileStore/tables/product_descriptions.csv\"\ndesc_df = spark.read.format(file_type) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"delimiter\", delimiter) \\\n  .option(\"escape\", \"\\\"\") \\\n  .load(file_location)\n\n# permanent_table_name = \"depot\"\n# df.write.format(\"parquet\").saveAsTable(permanent_table_name)"],"metadata":{"id":"vNN4V1yY8taF","colab_type":"code","colab":{}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["# Raw data exploration\n#### Our raw data includes 6 columns (in total 74067 observations) : \n1. Product UID\n2. Product Title\n3. Product Description \n4. Search Term \n5. Relevance Score\n6. id"],"metadata":{}},{"cell_type":"code","source":["alldata = title_df.join(desc_df, on=['product_uid'], how='left')\nalldata = alldata.withColumn('relevance', alldata.relevance.cast(FloatType()))\ndisplay(alldata.take(5))\nalldata.count()"],"metadata":{"colab_type":"code","id":"qMUvhlWC8tWF","colab":{"base_uri":"https://localhost:8080/","height":306},"executionInfo":{"user_tz":360,"timestamp":1574816018387,"elapsed":11766,"status":"ok","user":{"displayName":"wc Lin","photoUrl":"","userId":"01823782433820491239"}},"outputId":"e7a5e7c0-a46c-4fc4-9d88-c75b1c758a0f"},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["## Distinct products and search terms"],"metadata":{}},{"cell_type":"code","source":["## The number of distinct search terms\n## The number of distinct products \nprint (\"number of distinct search items\", alldata.select('search_term').distinct().count())\nprint (\"number of distinct products\", alldata.select('product_uid').distinct().count())"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["## The number of times that each product showed up in our data"],"metadata":{}},{"cell_type":"code","source":["## The number of products \nproduct_result = alldata.groupBy('product_uid').count().orderBy('count', ascending=False)\n## Try to use display\ndisplay(product_result)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["## Distribution of relevance scores"],"metadata":{}},{"cell_type":"code","source":["## A histogram plot on relevance \n## Shows how accurate the match is \nimport matplotlib.pyplot as plt\nvar = 'relevance'\nplot_data = alldata.select(var).toPandas()\nx= plot_data[var]\n\nbins =[0,0.5,1,1.5,2,2.5,3,3.5,4]\n\nhist, bin_edges = np.histogram(x,bins,weights=np.zeros_like(x) + 100. / x.size) # make the histogram\n\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(1, 1, 1)\n# Plot the histogram heights against integers on the x axis\nax.bar(range(len(hist)),hist,width=1,alpha=0.8,ec ='black',color = 'gold')\n\n# # Set the ticks to the middle of the bars\nax.set_xticks([0.5+i for i,j in enumerate(hist)])\n\n# Set the xticklabels to a string that tells us what the bin edges were\n#labels =['{}k'.format(int(bins[i+1]/1000)) for i,j in enumerate(hist)]\nlabels =['{}'.format(bins[i+1]) for i,j in enumerate(hist)]\nlabels.insert(0,'0')\nax.set_xticklabels(labels)\n#plt.text(-0.6, -1.4,'0')\nplt.xlabel(var)\nplt.ylabel('percentage')\nplt.show()\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["# Data preprocessing using Spark RDD APIs"],"metadata":{}},{"cell_type":"markdown","source":["### Word Tokenization, Change all words into lower case"],"metadata":{}},{"cell_type":"code","source":["# Change all words to lower case\ntokens_in_desc_list = alldata.select('product_description').rdd.flatMap(lambda x: x)\ntokens_in_desc_list = tokens_in_desc_list.collect()\ntokens_in_desc_list = [sent.lower() for sent in tokens_in_desc_list]\ntokens_in_desc_list = ' '.join(tokens_in_desc_list).split(' ')\n\ntokens_in_title_list = alldata.select('product_title').rdd.flatMap(lambda x: x)\ntokens_in_title_list = tokens_in_title_list.collect()\ntokens_in_title_list = [sent.lower() for sent in tokens_in_title_list]\ntokens_in_title_list = ' '.join(tokens_in_title_list).split(' ')"],"metadata":{"id":"FdrXvPObKM-e","colab_type":"code","colab":{}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["### Use the GLoVe method to obtain vector representations for words."],"metadata":{}},{"cell_type":"code","source":["## To change all words into vectors\n# File location and type\nfile_location = \"/FileStore/tables/glove_50d.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndf = df.toPandas()\ndf = df.set_index('index')\ndf['combined']= df.values.tolist()\ndf = df.reset_index()\nglove_dict = dict(zip(df['index'], df['combined']))"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# every token (word) has a unique vector representation\nprint(glove_dict['obama'])"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["# Text Preprocessing"],"metadata":{"id":"ir-GyNm4GLwx","colab_type":"text"}},{"cell_type":"code","source":["# --------------------------------------------------------------------------------------------------------\n# Spelling Checker\n# http://norvig.com/spell-correct.html\nfrom collections import Counter\n\ndef words(text): return re.findall(r'\\w+', text.lower())\nword_list = brand_list + attr_list + color_list + tokens_in_desc_list + tokens_in_title_list\n# words(open('big.txt').read())\n\n# tokens_in_search_list\n# stop_words = set(list(stop_words) + ['electic'])\nWORDS = Counter([w for w in word_list if w not in ['electic']])\n\ndef P(word, N=sum(WORDS.values())): \n    \"Probability of `word`.\"\n    return WORDS[word] / N\n\ndef correction(word): \n    \"Most probable spelling correction for word.\"\n    return max(candidates(word), key=P)\n\ndef candidates(word): \n    \"Generate possible spelling corrections for word.\"\n    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n\ndef known(words): \n    \"The subset of `words` that appear in the dictionary of WORDS.\"\n    return set(w for w in words if w in WORDS)\n\ndef edits1(word):\n    \"All edits that are one edit away from `word`.\"\n    letters    = 'abcdefghijklmnopqrstuvwxyz'\n    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n    deletes    = [L + R[1:]               for L, R in splits if R]\n    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n    inserts    = [L + c + R               for L, R in splits for c in letters]\n    return set(deletes + transposes + replaces + inserts)\n\ndef edits2(word): \n    \"All edits that are two edits away from `word`.\"\n    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n\ndef spell_correct(string):\n  res = [correction(word) for word in string.split(' ')]\n  return(' '.join(res))\n\nspell_correct_udf = F.udf(spell_correct, StringType())\n  \n# --------------------------------------------------------------------------------------------------------\n# text filtering\nfilters = [\n           gsp.strip_tags, \n           gsp.strip_punctuation,\n           gsp.strip_multiple_whitespaces, \n           gsp.strip_short, \n           gsp.remove_stopwords,\n           # gsp.stem_text,\n           # gsp.strip_numeric\n          ]\n\ndef clean_text(x):\n    s = x\n    s = s.lower()\n    s = utils.to_unicode(s)\n    for f in filters:\n        s = f(s)\n    return (s)\n\nclean_text_udf = F.udf(clean_text, StringType())\n\n# --------------------------------------------------------------------------------------------------------\n# word stemming\ndef stem_udf(token_list):\n  nltk.download('wordnet')\n  from nltk.stem import WordNetLemmatizer \n  lemmatizer = WordNetLemmatizer()\n  token_list = [lemmatizer.lemmatize(w) for w in token_list]\n  return(token_list)\n\nstem_udf = F.udf(stem_udf, ArrayType(StringType()))\n\n# --------------------------------------------------------------------------------------------------------\n# Split Concantenated Words\ndef split_concatenated_words(text):\n  return re.sub('([a-z]+)([A-Z]+)([a-z]+)', lambda matched: matched.group(1) + ' ' + matched.group(2) + matched.group(3), text)\nsplit_concatenated_words_udf = F.udf(split_concatenated_words, StringType())"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["### Cleaning \n- Split concatenated words\n- Remove useless info (i.e. stopwords, tag, multiple spaces, etc.)"],"metadata":{}},{"cell_type":"code","source":["texts = [\"concrete surfaceActual\", \"storesOnline\", 'informationRevives']\nfor i in texts:\n  print(i, '->', split_concatenated_words(i))"],"metadata":{"id":"YYHk91Wh2Uli","colab_type":"text"},"outputs":[],"execution_count":26},{"cell_type":"code","source":["alldata = alldata.withColumn('product_description', split_concatenated_words_udf(alldata.product_description))"],"metadata":{"id":"bFk4jO4SGKpV","colab_type":"code","colab":{}},"outputs":[],"execution_count":27},{"cell_type":"code","source":["print('punctuations,', '->', clean_text('punctuations,'))\nprint('hello  world', '->', clean_text('hello  world'))\nprint('an apple', '->', clean_text('an apple'))"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["# remove tags, punctuation, multiple_whitespaces, short words, stop words, lower\na = alldata.withColumn('product_title_list', clean_text_udf(alldata.product_title))\na = a.withColumn('search_term_list', clean_text_udf(a.search_term))\na = a.withColumn('product_desc_list', clean_text_udf(a.product_description))"],"metadata":{"id":"vJwtqO_G4S55","colab_type":"code","colab":{}},"outputs":[],"execution_count":29},{"cell_type":"code","source":["# rearrange column\na = a.select('id', 'product_uid', \n             'product_title', 'product_title_list', \n             'product_description', 'product_desc_list', \n             'search_term', 'search_term_list', 'relevance')"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["### Spelling Check\n\n1. Build a dictionary with correct words as keys and probability of a word as values\n2. Modify the misspelled words (insert letters, delete letters, replace letters, etc.) and see whether the modified word is in the dictionary (maximized probability)"],"metadata":{"id":"ItMzDqs72Zlp","colab_type":"text"}},{"cell_type":"code","source":["display(alldata.take(5))"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["# spelling check example\nspell_correct('electic')"],"metadata":{"id":"wlSghDmG2hoE","colab_type":"code","colab":{}},"outputs":[],"execution_count":33},{"cell_type":"code","source":["a = a.withColumn('product_title_list', spell_correct_udf(a.product_title_list))\na = a.withColumn('product_desc_list', spell_correct_udf(a.product_desc_list))\na = a.withColumn('search_term_list', spell_correct_udf(a.search_term_list))"],"metadata":{"id":"uJy-ekzn2hui","colab_type":"code","colab":{}},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["# Feature Engineering"],"metadata":{}},{"cell_type":"markdown","source":["### Helper functions for feature engineering"],"metadata":{}},{"cell_type":"code","source":["# Word Mover's Distance\ndef tokens_to_fracdict(tokens):\n    cntdict = defaultdict(lambda : 0)\n    for token in tokens:\n        cntdict[token] += 1\n    totalcnt = sum(cntdict.values())\n    return {token: float(cnt)/totalcnt for token, cnt in cntdict.items()}\n\ndef word_mover_distance_probspec(first_sent_tokens, second_sent_tokens):\n    first_sent_tokens  = [token for token in set(first_sent_tokens) if token in glove_dict]\n    second_sent_tokens = [token for token in set(second_sent_tokens) if token in glove_dict]\n    all_tokens = set(first_sent_tokens + second_sent_tokens)\n    if len(first_sent_tokens) == 0 or len(second_sent_tokens) == 0:\n      return -9999\n    \n    wordvecs = {token: glove_dict[token] for token in all_tokens}\n    print(wordvecs)\n    \n    # initial values for each token in a list\n    first_sent_buckets = tokens_to_fracdict(first_sent_tokens)\n    print(first_sent_buckets)\n    second_sent_buckets = tokens_to_fracdict(second_sent_tokens)\n    print(second_sent_buckets)\n\n    T = pulp.LpVariable.dicts('T_matrix', list(product(all_tokens, all_tokens)), lowBound=0)\n    \n    # define it as a minimization problem\n    prob = pulp.LpProblem('WMD', sense=pulp.LpMinimize)\n    \n    # the equation that we want to minimize - \n    # (distance b/w tokens) * \n    # (how much of word i in the first document - d travels to word j in the new document - d')\n    # therefore, we want to minimize the traveling distance.\n    prob += pulp.lpSum([T[token1, token2] * euclidean(wordvecs[token1], wordvecs[token2])\n                        for token1, token2 in product(all_tokens, all_tokens)])\n    \n    # add constraints\n    for token2 in second_sent_buckets:\n        prob += pulp.lpSum([T[token1, token2] for token1 in first_sent_buckets]) == second_sent_buckets[token2]\n    \n    for token1 in first_sent_buckets:\n        prob += pulp.lpSum([T[token1, token2] for token2 in second_sent_buckets]) == first_sent_buckets[token1]\n\n    prob.solve()\n    return prob\n\ndef word_mover_distance(first_sent_tokens, second_sent_tokens):\n    prob = word_mover_distance_probspec(first_sent_tokens, second_sent_tokens)\n    if prob == -9999:\n      return 20.0\n    elif pulp.value(prob.objective) is None:\n      return 0.0\n    else:\n      res = pulp.value(prob.objective) / len(second_sent_tokens)\n    return res\n\nword_mover_distance_udf = F.udf(word_mover_distance, FloatType())\n\n# --------------------------------------------------------------------------------------------------------\n# Euclidean Distance\ndef euclidean_distance(first_tokens, second_tokens):\n\n  default = np.array([0] * 50)\n  first_vectors  = [glove_dict[token] if token in glove_dict else default for token in set(first_tokens)]\n  second_vectors = [glove_dict[token] if token in glove_dict else default for token in set(second_tokens)]\n  \n  first_vectors = np.array([sum(x) for x in zip(*first_vectors)]) / len(first_vectors)\n  second_vectors = np.array([sum(x) for x in zip(*second_vectors)]) / len(second_vectors)\n\n  return(euclidean(first_vectors, second_vectors))\n\neuclidean_distance_udf = F.udf(euclidean_distance, FloatType())\n\n# --------------------------------------------------------------------------------------------------------\n# Cosine Distance\ndef cos_sim(a,b):\n    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n\ndef cos_sim_distance(first_tokens, second_tokens):\n\n  default = np.array([0] * 50)\n  first_vectors  = [glove_dict[token] if token in glove_dict else default for token in set(first_tokens)]\n  second_vectors = [glove_dict[token] if token in glove_dict else default for token in set(second_tokens)]\n\n  first_vectors = np.array([sum(x) for x in zip(*first_vectors)]) / len(first_vectors)\n  second_vectors = np.array([sum(x) for x in zip(*second_vectors)]) / len(second_vectors)\n\n  return(cos_sim(first_vectors, second_vectors))\n\ncos_sim_distance_udf = F.udf(cos_sim_distance, FloatType())\n\n# --------------------------------------------------------------------------------------------------------\n# Extract Noun Phrases\nstop_words=set(stopwords.words('english'))\n\ndef extractPhraseFunct(x):\n    def leaves(tree):\n        \"\"\"Finds NP (nounphrase) leaf nodes of a chunk tree.\"\"\"\n        for subtree in tree.subtrees(filter = lambda t: t.label()=='NP'):\n            yield subtree.leaves()\n    \n    def get_terms(tree):\n        for leaf in leaves(tree):\n            term = [w for w,t in leaf if not w in stop_words]\n            yield term\n\n    sentence_re = r'''(?x)          # set flag to allow verbose regexps\n        (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n      | \\w+(?:[\\+|-]\\w+)*       # words with optional internal plus\n      | \\$?\\d+(?:\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n      | \\.\\.\\.              # ellipsis\n      | [][.,;\"'?():_`-]    # these are separate tokens; includes ], [\n    '''\n\n    grammar = r\"\"\"\n    NBAR:\n        {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n        \n    NP:\n        {<NBAR>}\n        {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n    \"\"\"\n    chunker = nltk.RegexpParser(grammar)\n    tokens = nltk.regexp_tokenize(x,sentence_re)\n    postoks = nltk.tag.pos_tag(tokens) #Part of speech tagging \n    tree = chunker.parse(postoks) #chunking\n    terms = get_terms(tree)\n    temp_phrases = []\n    for term in terms:\n        if len(term):\n            temp_phrases.append(' '.join(term))\n    \n    finalPhrase = [w for w in temp_phrases if w] #remove empty lists\n    finalPhrase = ' '.join(finalPhrase)\n\n    return finalPhrase\n\nextractPhraseFunct_udf = F.udf(extractPhraseFunct, StringType())\n\n# --------------------------------------------------------------------------------------------------------\n# Brand Extraction\nfile_location = \"/FileStore/tables/attr_brands.txt\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"false\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\nbrand_list = df.select('_c0').rdd.flatMap(lambda x: x).collect()\nbrand_list = [token.lower() for token in brand_list]\n\n# in brand list\ndef in_brand_list(title_string):\n  title_list = title_string.lower().split(' ')[:4]\n\n  for i in range(1, 5)[::-1]:\n    title = ' '.join(title_list[:i])\n    \n    if title in set(brand_list):\n      return(title)\n    \n  return('none')\n  \nin_brand_list_udf = F.udf(in_brand_list, StringType())\n\n# share brand\ndef share_brand(brand, search_brand):\n  if search_brand == 'none':\n    return(0)\n  elif search_brand not in brand:\n    return(0)\n  else:\n    return(1)\n\nshare_brand_udf = F.udf(share_brand, IntegerType())\n\n# --------------------------------------------------------------------------------------------------------\n# Attribute Extraction\nfile_location = \"/FileStore/tables/most_common_attrs.txt\"\nfile_type = \"csv\"\n\ndf = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\nattr_list = df.select('_c0').rdd.flatMap(lambda x: x).collect()\nattr_list = [re.findall(r'[a-zA-Z]+', sent.lower()) for sent in attr_list]\nattr_list = [item for sublist in attr_list for item in sublist]\n\n# --------------------------------------------------------------------------------------------------------\n# Color Extraction\nfile_location = \"/FileStore/tables/most_common_colors.txt\"\nfile_type = \"csv\"\n\ndf = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ncolor_list = df.select('_c0').rdd.flatMap(lambda x: x).collect()\ncolor_list = [i.split()[1] for i in color_list]\n\n# in color list\ndef in_color_list(string):\n  l = string.lower().split(' ')\n  res = []\n\n  for token in l:\n    if token in color_list:\n      res.append(token)\n  \n  if len(res) > 0:\n    return(res)\n  else:\n    return(['none'])\n  \nin_color_list_udf = F.udf(in_color_list, ArrayType(StringType()))\n\n# share color\ndef share_color(color, search_color):\n  if ('none' in color) or ('none' in search_color):\n    return(0)\n  elif len(set(color) & set(search_color)) > 0:\n    return(1)\n  else:\n    return(0)\n\nshare_color_udf = F.udf(share_color, IntegerType())"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["### Brand / Color Extraction"],"metadata":{"id":"d-jEH8czDOxg","colab_type":"text"}},{"cell_type":"code","source":["a = a.withColumn('brand', in_brand_list_udf(a.product_title))\na = a.withColumn('search_brand', in_brand_list_udf(a.search_term_list))\na = a.withColumn('share_brand', share_brand_udf(a.brand, a.search_brand))"],"metadata":{"id":"0eddO5sVC1Qe","colab_type":"code","colab":{}},"outputs":[],"execution_count":39},{"cell_type":"code","source":["p1 = a.groupBy('share_brand').agg({'relevance':'avg'})\ndisplay(p1)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["a = a.withColumn('color', in_color_list_udf(a.product_desc_list))\na = a.withColumn('search_color', in_color_list_udf(a.search_term_list))\na = a.withColumn('share_color', share_color_udf(a.color, a.search_color))"],"metadata":{"id":"Dp8IIuOiQxos","colab_type":"code","colab":{}},"outputs":[],"execution_count":41},{"cell_type":"code","source":["p2 = a.groupBy('share_color').agg({'relevance':'avg'})\ndisplay(p2)"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["### Lemmatization"],"metadata":{}},{"cell_type":"code","source":["# split into list\na = a.withColumn('product_title_list', F.split(a.product_title_list, ' '))\na = a.withColumn('product_desc_list', F.split(a.product_desc_list, ' '))\na = a.withColumn('search_term_list', F.split(a.search_term_list, ' '))\naa = a.withColumn('product_title_list', stem_udf(a.product_title_list))\naa = aa.withColumn('product_desc_list', stem_udf(aa.product_desc_list))\naa = aa.withColumn('search_term_list', stem_udf(aa.search_term_list))\naa.cache()"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["### Euclidean / Cosine / Word Mover's Distance"],"metadata":{"id":"rjaY3k6nysAd","colab_type":"text"}},{"cell_type":"markdown","source":["#### Euclidean / Cosine\n\n1. Transforms each document (search term, description, title) into a vector using the average of all words in the document\n2. Calculate distance"],"metadata":{"id":"dEGEcAmrZkxn","colab_type":"text"}},{"cell_type":"code","source":["aaa = aa.withColumn('search_desc_euclidean', euclidean_distance_udf(aa.search_term_list, aa.product_desc_list))\naaa = aaa.withColumn('search_title_euclidean', euclidean_distance_udf(aaa.search_term_list, aaa.product_title_list))"],"metadata":{"id":"VrII9eX4yrSZ","colab_type":"code","colab":{}},"outputs":[],"execution_count":47},{"cell_type":"code","source":["aaaa  = aaa.withColumn('search_title_cosine', cos_sim_distance_udf(aaa.search_term_list, aaa.product_title_list))\naaaaa = aaaa.withColumn('search_desc_cosine', cos_sim_distance_udf(aaaa.search_term_list, aaaa.product_desc_list))"],"metadata":{"id":"d0cgzv4vZgYU","colab_type":"code","colab":{}},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["#### Word Mover's Distance (WMD)\nCalculate the shortest amount of distance needed to move the words from one side into the other. The smaller the distance is, the closer they are. Take, for example, two headlines (these two headlines say the same thing in completely different words):\n1. Obama speaks to the media in Illinois\n2. The President greets the press in Chicago\n\nmove the words from one side into the other\n- Obama -> President (distance: 0.45)\n- speaks -> greets (distance: 0.24)\n- media -> press (distance: 0.20)\n- Illinois -> Chicago (distance: 0.18)\n\nThe total distance = 1.07"],"metadata":{"id":"ByVu23-5_xv8","colab_type":"text"}},{"cell_type":"code","source":["aaaaaa = aaaaa.withColumn('search_desc_wmd', word_mover_distance_udf('product_desc_list', 'search_term_list'))\naaaaaa = aaaaaa.withColumn('search_title_wmd', word_mover_distance_udf('product_title_list', 'search_term_list'))"],"metadata":{"id":"6Y_ObcTWeNl4","colab_type":"code","colab":{}},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":["## Percentage of words from the search term that match the title/decription\n\n**Search term**: ***angle*** bracket\n\n**Product title**: Simpson Strong-Tie 12-Gauge ***Angle***\n\n**Feature value**: 0.50"],"metadata":{}},{"cell_type":"code","source":["def word_match_percentage(search_term_list, title_or_desc_term_list):\n  return len(set(search_term_list) & set(title_or_desc_term_list)) / len(set(title_or_desc_term_list))\nword_match_percentage_udf = F.udf(word_match_percentage, DoubleType())"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["aaaaaaa = aaaaaa.withColumn('search_title_match', word_match_percentage_udf(aaaaaa.search_term_list, aaaaaa.product_title_list))\naaaaaaa = aaaaaaa.withColumn('search_desc_match', word_match_percentage_udf(aaaaaaa.search_term_list, aaaaaaa.product_desc_list))"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["aaaaaaa.cache()\ndisplay(aaaaaaa.take(5))"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"markdown","source":["# Final Dataframe before Modelling :\n## Ten features and one response variable\n- share_brand<br>\n- share_color<br>\n- search_title_euclidean<br>\n- search_desc_euclidean<br>\n- search_title_cosine<br>\n- search_desc_cosine<br>\n- search_title_wmd<br>\n- search_desc_wmd<br>\n- search_title_match<br>\n- search_desc_match<br>\n- relevance<br>"],"metadata":{}},{"cell_type":"code","source":["# Keep relevant columns\nfinal_df = aaaaaaa.select('share_brand', 'share_color', 'search_desc_euclidean', 'search_title_euclidean', 'search_title_cosine', \n                         'search_desc_cosine', 'search_title_wmd', 'search_desc_wmd', 'search_title_match','search_desc_match' ,'relevance')"],"metadata":{"id":"L0aQhHLE06uH","colab_type":"code","colab":{}},"outputs":[],"execution_count":56},{"cell_type":"code","source":["final_df = final_df.na.fill(0)\nfinal_df.cache()\ndisplay(final_df.take(5))"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["final_df.printSchema()"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["## zero means no share\ndisplay(final_df.select(\"share_brand\"))"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["display(final_df.select(\"share_color\"))"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"markdown","source":["# Modeling\nWith relavance as the response variable, we run three machine learning models on the ten features we created. \n- Linear regression<br>\n- Random Forest<br>\n- Gradient Boosting<br>\n## Evaluation\n- RMSE<br>"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler \nassembler = VectorAssembler(inputCols=['search_desc_euclidean', 'search_title_euclidean', \n                                       'search_title_cosine', 'search_desc_cosine', \n                                       'search_title_wmd', 'search_desc_wmd', 'search_title_match','search_desc_match',\n                                       'share_brand', 'share_color'], \n                            outputCol=\"features\")"],"metadata":{"id":"NDcgvAyg06l8","colab_type":"code","colab":{}},"outputs":[],"execution_count":62},{"cell_type":"code","source":["dataset = assembler.transform(final_df)\ndataset = dataset.select(\"features\", \"relevance\")"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"code","source":["splits = dataset.randomSplit([0.7, 0.3], 123)\ntrain_df = splits[0]\ntest_df = splits[1]"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"code","source":["# Import LinearRegression class\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.regression import DecisionTreeRegressor\nfrom pyspark.ml.regression import GBTRegressor\n# Define GBT, linear regression and decision tree algorithm\nlr1 = GBTRegressor(featuresCol = 'features', labelCol='relevance')\nlr2 = LinearRegression(featuresCol = 'features', labelCol='relevance', maxIter=10, regParam=0.0, elasticNetParam=0.8)\nlr3 = DecisionTreeRegressor(featuresCol = 'features', labelCol='relevance')"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"code","source":["modelA = lr1.fit(train_df)"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"code","source":["modelB = lr2.fit(train_df)"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":["modelC = lr3.fit(train_df)"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"markdown","source":["# Evaluation"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\npred_gbt = modelA.transform(test_df)\nevaluator = RegressionEvaluator(labelCol=\"relevance\", \n                                predictionCol=\"prediction\", \n                                metricName=\"rmse\")\nevaluator.evaluate(pred_gbt)"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"code","source":["pred_linear = modelB.transform(test_df)\nevaluator = RegressionEvaluator(labelCol=\"relevance\", \n                                predictionCol=\"prediction\", \n                                metricName=\"rmse\")\nevaluator.evaluate(pred_linear)"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"code","source":["pred_tree = modelC.transform(test_df)\nevaluator = RegressionEvaluator(labelCol=\"relevance\", \n                                predictionCol=\"prediction\", \n                                metricName=\"rmse\")\nevaluator.evaluate(pred_tree)"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"markdown","source":["# Conclusion\nThe baseline models we constructed showed satisfying results that proved our concept. We used the Root Mean Square Error (RMSE) as our metric. All three models led to small RMSEs. Comparing the RMSE generated by the three models, the Gradient boost tree model performed the best with the minimum rmse value of 0.4976. At the begining, we only used euclidean distance as our feature and got a high rmse and low R-square. To improve our model, we added more features such as consine distance, word mover distance, whether the brand from search term match with product title, whether the color from search term match with product description. After import more features into the models, rmse reduced by 30%.\n\n\n# Discussion\nIn this project, we implemented NLP steps to preprocess data for example, tokenization, lemmatiztaion, word embedidng and so on. We then engineered features and utilized machine learning algorithms, including linear regression, gradient boosted tree and random forest to predict the relavance score between search term and product descriptions. \nIn this baseline model that we constructed we used pre-trained word embedding method GLoVe. To further improve our model, we could adapt contextulized word embedding method, for example BERT, or better tune our models. \nSince this pipeline could be extended to calculate or predict the relavance between any texts. It could be utilized to many aspects, for example, finding the product match, converting comments to scores, recommendation system, and so on."],"metadata":{"id":"lp8x-yOIFVZT","colab_type":"code","colab":{}}},{"cell_type":"markdown","source":["# Reference\n\n- https://vene.ro/blog/word-movers-distance-in-python.html\n- https://medium.com/towards-artificial-intelligence/multi-class-text-classification-using-pyspark-mllib-doc2vec-dbfcee5b39f2\n- http://norvig.com/spell-correct.html"],"metadata":{"id":"-73205SQAmsE","colab_type":"code","colab":{}}}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.3","nbconvert_exporter":"python","file_extension":".py"},"name":"STAT480_Group_Project_colab","notebookId":4471727769864755,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"colab":{"name":"STAT480 Group Project.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}
